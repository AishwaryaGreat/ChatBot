{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Building the RAG Model"
      ],
      "metadata": {
        "id": "qudGOj0uGter"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Er3c8JimUI1N"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from cohere import Client\n",
        "import pdfplumber\n",
        "import streamlit as st\n",
        "import cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OccqWCVGXWnv",
        "outputId": "097db484-e5f3-4cc1-b2a9-259ab8ed831b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-19 11:32:14.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 11:32:14.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 11:32:14.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 11:32:14.065 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 11:32:14.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "# Prompt user to upload a PDF file\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF file\", type=\"pdf\")\n",
        "\n",
        "# Ensure there's an uploaded file before proceeding\n",
        "if uploaded_file is not None:\n",
        "    # Use pdfplumber to extract text from the uploaded PDF\n",
        "    def extract_text_from_pdf(uploaded_file):\n",
        "        with pdfplumber.open(uploaded_file) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"  # Extract text from each page\n",
        "        return text\n",
        "\n",
        "    # Extract and display the text from the PDF\n",
        "    document_text = extract_text_from_pdf(uploaded_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI_VyZ-MXhmI",
        "outputId": "064d3216-9955-4659-ea75-456f825859fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 8 Biology is the youngest of the formalised disciplines of natural\n",
            "Human Health and Disease science\n",
            "[ 1.00957649e-03  4.38434370e-02  7.38708116e-03  1.76462550e-02\n",
            " -1.16258757e-02  5.03440313e-02 -8.39617550e-02  1.39322951e-01\n",
            " -6.99806809e-02  8.96712393e-02 -1.97667554e-02  1.04239595e-03\n",
            " -6.46360591e-02  4.88311462e-02 -7.83217326e-02 -1.49432011e-02\n",
            " -1.17612518e-01  6.37972448e-03 -4.76321280e-02  1.72314160e-02\n",
            " -3.37933600e-02  1.48807392e-01  1.04132853e-01  1.52420951e-02\n",
            " -3.35123464e-02 -3.20260897e-02  3.42959613e-02 -8.30849335e-02\n",
            " -5.41196801e-02 -1.25355367e-03  2.09651329e-02  5.59218600e-02\n",
            "  6.72592893e-02 -1.73090771e-02 -2.53563710e-02  4.94369827e-02\n",
            "  9.40992013e-02 -4.94890511e-02  1.58186257e-02  5.40051013e-02\n",
            " -3.85849811e-02 -6.23704679e-02 -4.09939699e-02  1.22355148e-02\n",
            "  3.30831818e-02  1.14591187e-02  6.31134259e-03 -2.63138991e-02\n",
            "  2.24546436e-02 -9.72576626e-03 -3.76431532e-02 -4.18561697e-02\n",
            " -8.86348560e-02  5.94508126e-02  3.35745588e-02  3.53903621e-02\n",
            " -4.43163440e-02 -1.20974980e-01 -6.29373416e-02 -4.22276519e-02\n",
            "  7.93678313e-03 -2.91463379e-02 -2.94599240e-03  7.93360621e-02\n",
            "  4.30520847e-02 -9.13973600e-02  2.05654241e-02  8.28590523e-03\n",
            " -2.58525461e-02  6.11151084e-02  1.01102320e-02 -3.27658504e-02\n",
            " -3.63316350e-02  8.31159279e-02  5.47078364e-02  1.33108320e-02\n",
            " -1.40728354e-02  2.88301185e-02  5.93053550e-02 -8.99544656e-02\n",
            " -6.77860458e-04 -3.30817960e-02  1.19571462e-01  3.20974961e-02\n",
            " -2.71119382e-02 -1.97281446e-02 -2.36448515e-02  2.19165422e-02\n",
            " -1.14504807e-01 -2.39093858e-03 -9.11500677e-03 -2.00562291e-02\n",
            "  6.53775856e-02  6.30297512e-02 -7.10297674e-02  3.76544669e-02\n",
            " -1.35899475e-02 -6.24510013e-02  9.99565423e-03 -1.00999209e-03\n",
            " -1.18827231e-01 -5.00611886e-02 -1.43389404e-02  8.66406485e-02\n",
            " -4.12842073e-03 -3.47863287e-02 -2.97593465e-03 -6.84756264e-02\n",
            "  2.72476561e-02 -1.52385831e-02  3.59826698e-03  4.55842651e-02\n",
            " -4.08830456e-02 -1.05593121e-02  3.77580002e-02  2.50345916e-02\n",
            "  4.72375453e-02 -3.31810564e-02  8.14098194e-02 -6.00256585e-02\n",
            " -5.14592826e-02 -2.11666971e-02 -1.29395695e-02 -3.57192047e-02\n",
            "  2.56461650e-02  1.86125226e-02 -1.10701350e-02 -5.58717130e-33\n",
            "  5.06566279e-02  5.56622492e-03  7.00587332e-02  3.95162702e-02\n",
            " -5.18629514e-03  1.61749143e-02  2.27005929e-02 -1.02501675e-01\n",
            "  2.44834088e-02 -1.92198791e-02  7.24803796e-03 -6.52057305e-02\n",
            "  1.57087259e-02 -5.03237825e-03 -3.89792919e-02  4.41989787e-02\n",
            " -3.62542756e-02  7.94993937e-02 -5.35682105e-02 -1.19804991e-02\n",
            " -2.02372186e-02 -3.82144609e-03  5.71993180e-03 -8.28031823e-02\n",
            "  2.46741325e-02  6.87016472e-02 -1.30712250e-02 -3.41708697e-02\n",
            "  6.17167689e-02 -1.02263913e-02 -2.48512384e-02 -3.23407026e-03\n",
            " -8.73773694e-02 -5.36872931e-02 -1.78701580e-02 -3.53156514e-02\n",
            "  4.15958464e-02 -8.81998986e-03  1.97205003e-02 -5.20249642e-03\n",
            "  1.61296800e-02 -7.84175750e-03  3.12304627e-02 -6.86057061e-02\n",
            "  9.38995183e-02  4.53596078e-02 -1.60351936e-02  2.51251296e-03\n",
            "  1.23976050e-02  2.27930285e-02  1.01766028e-02 -1.06265526e-02\n",
            "  1.14146173e-01 -4.48574461e-02  6.55964538e-02  1.64047889e-02\n",
            " -2.02046819e-02  5.51856570e-02 -7.00288713e-02  5.46714524e-03\n",
            "  8.76368433e-02  1.08806618e-01 -3.34611684e-02 -2.93578822e-02\n",
            " -6.23478275e-03 -3.32024172e-02 -7.00177178e-02 -9.70330238e-02\n",
            "  4.83527072e-02  1.56822987e-02 -4.01189998e-02  4.32244986e-02\n",
            " -6.24289103e-02 -1.21179167e-02  4.07869294e-02 -3.47504690e-02\n",
            " -7.21124373e-03 -6.24995381e-02 -6.73645511e-02  7.15807155e-02\n",
            " -4.70278971e-02  1.36903264e-02 -9.16531980e-02  5.26098907e-02\n",
            " -5.11916205e-02  8.26583151e-03  7.38126859e-02  3.14327423e-03\n",
            " -1.52202006e-02 -5.63473590e-02  5.25440415e-03 -4.03029323e-02\n",
            "  3.23987529e-02  9.14847851e-02 -2.72897631e-03 -3.44300028e-35\n",
            " -9.89295193e-04 -2.94147078e-02 -3.98363322e-02  6.09410275e-03\n",
            "  7.49779046e-02  1.04981959e-02 -4.25752020e-03  2.36687367e-03\n",
            "  1.74550787e-02 -1.08694080e-02  7.79479519e-02  3.34077142e-02\n",
            "  4.29056026e-02 -2.99258567e-02  3.42001244e-02  3.30037577e-03\n",
            " -8.64650682e-02  8.32717121e-03 -6.71017691e-02 -2.47542020e-02\n",
            " -3.20856459e-03  6.69840053e-02 -9.42946821e-02  1.75248012e-02\n",
            "  3.15797105e-02 -3.06756645e-02  3.63344653e-03  6.35942891e-02\n",
            " -3.38295065e-02 -3.05199660e-02  1.03279948e-02  4.62246463e-02\n",
            " -4.17068079e-02 -8.88731331e-03 -2.31360365e-02 -2.53267065e-02\n",
            "  1.54775335e-02 -9.74198803e-02  5.79432622e-02 -6.97053745e-02\n",
            "  9.97466370e-02  1.65567212e-02 -6.37089387e-02  8.60203803e-03\n",
            "  2.51952261e-02  4.69772890e-02  6.01182058e-02  7.95660019e-02\n",
            " -4.72692139e-02  5.49847223e-02  4.36579883e-02 -1.49385361e-02\n",
            " -3.53735276e-02 -5.24111465e-02  6.24240190e-02 -1.85519066e-02\n",
            " -4.27743606e-02 -7.28081688e-02  5.66073656e-02  1.81851555e-02\n",
            "  4.30725701e-02  1.05077550e-02 -3.46892104e-02  9.37085897e-02\n",
            " -9.30275321e-02  2.87752002e-02 -5.60195632e-02  3.07453517e-02\n",
            " -7.02354917e-03 -6.88210595e-03 -6.74756989e-02  4.05704677e-02\n",
            " -7.83284679e-02  1.38544999e-02  4.93912324e-02 -5.19702546e-02\n",
            " -4.98646684e-02 -2.03227662e-02  4.13925853e-03  9.60028265e-03\n",
            " -5.04619665e-02 -9.15611237e-02 -3.24890874e-02 -1.58074517e-02\n",
            " -2.68278556e-04 -4.02908698e-02  4.19859216e-02 -6.63805902e-02\n",
            "  8.15542974e-03  5.49426414e-02 -6.31661117e-02 -6.36474416e-02\n",
            " -1.03428572e-01  4.11765240e-02 -7.84225985e-02 -2.47153729e-08\n",
            "  1.29342631e-01 -4.06666985e-03  5.60648441e-02 -9.91650596e-02\n",
            "  6.33150572e-03  5.43861575e-02 -5.60768060e-02  9.44740176e-02\n",
            "  5.08342870e-02  5.91798462e-02  1.27384411e-02  1.10427231e-01\n",
            "  7.36809149e-02  4.26927954e-02  4.52354997e-02 -2.23315507e-02\n",
            "  5.81246987e-03 -8.48320685e-03 -5.61717749e-02 -2.57011205e-02\n",
            "  3.48730162e-02  9.01680626e-03  3.27761024e-02 -2.58905813e-02\n",
            "  1.82185043e-02 -3.93141322e-02  2.99779270e-02 -7.79210241e-04\n",
            " -1.43905785e-02 -1.38896570e-01  1.96578633e-02  8.62032548e-02\n",
            " -5.43184280e-02  2.18996257e-02 -3.78175005e-02 -1.72270816e-02\n",
            "  7.72191733e-02  1.70659274e-02 -3.87978107e-02  1.78685505e-02\n",
            "  3.59118916e-03 -2.80621108e-02 -8.98190364e-02  6.02416433e-02\n",
            " -1.71612557e-02 -5.31731173e-02  4.93684821e-02  5.22053503e-02\n",
            "  2.81971730e-02 -8.72094929e-02 -7.68519044e-02  3.30175483e-03\n",
            "  3.68839167e-02 -6.53982237e-02 -9.19590071e-02  5.96281104e-02\n",
            " -2.21058931e-02 -1.98012423e-02 -8.37580115e-02 -8.65966454e-02\n",
            "  7.30638877e-02  2.07698662e-02  7.48287961e-02 -7.72885233e-03]\n"
          ]
        }
      ],
      "source": [
        "# Generate Document Embeddings\n",
        "# create embeddings from the text extracted from the PDF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate sentence-level embeddings for the text\n",
        "sentences = document_text.split(\".\")  # Split text into sentences\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "# Example usage: print one sentence and its corresponding embedding\n",
        "print(sentences[0])\n",
        "print(sentence_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csj4abRDY8B-",
        "outputId": "b2696aa8-556d-4360-85a5-6b70ad3b7127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings successfully uploaded to Pinecone!\n"
          ]
        }
      ],
      "source": [
        "# Set your Pinecone API key (ensure to replace with your actual API key)\n",
        "api_key = '3df05ff6-0236-4da1-b8f5-f803a6d00eb1'\n",
        "\n",
        "# Initialize the Pinecone instance\n",
        "pc = Pinecone(api_key=api_key)\n",
        "\n",
        "# Define the index name and embedding dimension\n",
        "index_name = 'document-embeddings'\n",
        "embedding_dim = sentence_embeddings.shape[1]  # Ensure sentence_embeddings is already defined\n",
        "\n",
        "# Check if the index already exists, if not, create it\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dim,\n",
        "        metric='euclidean',  # You can adjust the metric based on your needs\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'  # You can specify your region\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Upload the embeddings to the index\n",
        "for i, embedding in enumerate(sentence_embeddings):\n",
        "    index.upsert([(f'sentence-{i}', embedding)])\n",
        "\n",
        "print(\"Embeddings successfully uploaded to Pinecone!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vcb3yDuMZdak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Function to find the most similar question\n",
        "def get_most_relevant_sentence(query, model, sentences, sentence_embeddings):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # Calculate cosine similarity with each sentence embedding\n",
        "    similarities = util.cos_sim(query_embedding, sentence_embeddings)\n",
        "\n",
        "    # Get the index of the most similar sentence\n",
        "    best_idx = np.argmax(similarities)\n",
        "\n",
        "    return sentences[best_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWkRlUVvasO3",
        "outputId": "71ca7482-a058-42e2-fa04-a9186b466153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant sentence:  Such disease-\n",
            "causing organisms are called pathogens\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "user_query = \"What are pathogens\"\n",
        "most_relevant_sentence = get_most_relevant_sentence(user_query, model, sentences, sentence_embeddings)\n",
        "print(f\"Most relevant sentence: {most_relevant_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQE1_JxobUtJ",
        "outputId": "f69f197e-25ec-4385-a0d0-89c0b032a214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated answer: Disease-causing organisms are called pathogens.\n"
          ]
        }
      ],
      "source": [
        "# Cohere API for Generating a Response\n",
        "# To generate more comprehensive answers, use a generative model like Cohere.\n",
        "# Initialize the Cohere client\n",
        "co = cohere.Client('eiVpA3k8j4oQXtOqRsHGWaW3tLHJqcaoMlkdGfG8')\n",
        "\n",
        "def generate_answer(context, query):\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # Generate a response using Cohere\n",
        "    response = co.generate(\n",
        "        model='command-xlarge-nightly',\n",
        "        prompt=prompt,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.generations[0].text\n",
        "\n",
        "# Example usage\n",
        "context = most_relevant_sentence\n",
        "answer = generate_answer(context, user_query)\n",
        "print(f\"Generated answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Interactive QA Bot Interface"
      ],
      "metadata": {
        "id": "vhP8YxNoG5pl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTS2RVeydElg",
        "outputId": "0d92c0a9-5ddd-4e51-8460-30eaaf357577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-19 12:19:19.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.757 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "# Interactive Interface Using Streamlit\n",
        "# Build Frontend Using Streamlit\n",
        "# Create an interface where users can upload PDFs and ask questions\n",
        "# Import the libraries\n",
        "import streamlit as st\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pdfplumber\n",
        "import pinecone\n",
        "import cohere\n",
        "\n",
        "# Load sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone_client = pinecone.Pinecone(api_key='3df05ff6-0236-4da1-b8f5-f803a6d00eb1')  # Replace with your Pinecone API key\n",
        "index = pinecone_client.Index('document-embeddings')\n",
        "\n",
        "# Initialize Cohere\n",
        "co = cohere.Client('eiVpA3k8j4oQXtOqRsHGWaW3tLHJqcaoMlkdGfG8')  # Replace with your Cohere API key\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(file):\n",
        "    with pdfplumber.open(file) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"QA Bot: Ask Questions from Your PDF\")\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF\", type=\"pdf\")\n",
        "\n",
        "if uploaded_file:\n",
        "    document_text = extract_text_from_pdf(uploaded_file)\n",
        "    st.write(\"Document uploaded successfully!\")\n",
        "\n",
        "    # Process document for embeddings\n",
        "    sentences = document_text.split(\".\")\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    # Handle user queries\n",
        "    user_query = st.text_input(\"Ask a question:\")\n",
        "\n",
        "    if user_query:\n",
        "        # Find the most relevant sentence\n",
        "        relevant_sentence = get_most_relevant_sentence(user_query, model, sentences, sentence_embeddings)\n",
        "        st.write(f\"Relevant sentence: {relevant_sentence}\")\n",
        "\n",
        "        # Generate a full answer using Cohere\n",
        "        answer = generate_answer(relevant_sentence, user_query)\n",
        "        st.write(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add multiple query handling\n",
        "user_query = st.text_input(\"Ask a question:\")\n",
        "\n",
        "if user_query:\n",
        "    query_embedding = model.encode(user_query)\n",
        "    relevant_docs = retrieve_relevant_docs(query_embedding)\n",
        "    context = ' '.join([doc['metadata']['text'] for doc in relevant_docs['matches']])\n",
        "\n",
        "    # Generate answer using Cohere\n",
        "    answer = generate_answer(context, user_query)\n",
        "    st.write(f\"Answer: {answer}\")\n",
        "\n",
        "    # Show relevant document segments\n",
        "    st.write(\"Relevant document segments:\")\n",
        "    for doc in relevant_docs['matches']:\n",
        "        st.write(doc['metadata']['text'])"
      ],
      "metadata": {
        "id": "7ZMO9Zre-isc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a1d3c5-fda6-48e3-c638-73be94fdb9c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-19 12:19:19.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.778 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.781 Session state does not function when running a script without `streamlit run`\n",
            "2024-10-19 12:19:19.782 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-10-19 12:19:19.783 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}